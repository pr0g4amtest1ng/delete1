This is an attempt to replicate the following paper as the hyperparameter link is not working in the paper.

arXiv:1302.4389_ **[stat.ML]**

.. _arXiv:1302.4389: https://arxiv.org/abs/1302.4389

======================
Hyperparameters Tuning
======================

--------
Training
--------

I've set by mistakenly dropout as 0.2. Somewhere I've read dropout should be less for input
channels. But I performed less dropout on weight vectors.

**Learning rate**: 0.001

+--------+------------+-------------------------+-------------------------+---------+--------+
|        |            |       Layer1            |       Layer2            |         |        |
| Epochs | Batch size +------------+------------+------------+------------+ Accuracy|  Loss  |
|        |            |  Number of |  Number of |  Number of |  Number of |   (%)   |        |
|        |            |   layers   |   Neurons  |   layers   |   Neurons  |         |        |
+========+============+============+============+============+============+=========+========+
|    5   |     64     |     3      |    512     |      2     |     10     |  93.31  | 1.6068 |
+--------+------------+------------+------------+------------+------------+---------+--------+
|    5   |     64     |     3      |    1024    |      2     |     10     |  93.87  | 1.5971 |
+--------+------------+------------+------------+------------+------------+---------+--------+
|    5   |     64     |     4      |    2048    |      2     |     10     |  94.31  | 1.5901 |
+--------+------------+------------+------------+------------+------------+---------+--------+
|    5   |     32     |     4      |    2048    |      2     |     10     |  94.75  | 1.5656 |
+--------+------------+------------+------------+------------+------------+---------+--------+

* After removing batch normalization from second maxout and adding normalization to input

+--------+------------+-------------------------+-------------------------+---------+--------+
|        |            |       Layer1            |       Layer2            |         |        |
| Epochs | Batch size +------------+------------+------------+------------+ Accuracy|  Loss  |
|        |            |  Number of |  Number of |  Number of |  Number of |   (%)   |        |
|        |            |   layers   |   Neurons  |   layers   |   Neurons  |         |        |
+========+============+============+============+============+============+=========+========+
|    5   |     64     |     3      |    1024    |      2     |     10     |  94.38  | 1.5355 |
+--------+------------+------------+------------+------------+------------+---------+--------+
|    5   |     64     |     3      |    2048    |      2     |     10     |  94.65  | 1.5307 |
+--------+------------+------------+------------+------------+------------+---------+--------+
|    5   |     64     |     4      |    1024    |      2     |     10     |  94.33  | 1.5242 |
+--------+------------+------------+------------+------------+------------+---------+--------+
|    5   |     64     |     4      |    2048    |      2     |     10     |  94.33  | 1.5911 |
+--------+------------+------------+------------+------------+------------+---------+--------+

----------
Validation
----------

+---------+------------+-------------------------+-------------------------+---------+--------+
|         |            |       Layer1            |       Layer2            |         |        |
|Training | Batch size +------------+------------+------------+------------+ Accuracy|  Loss  |
| Epochs  |            |  Number of |  Number of |  Number of |  Number of |   (%)   |        |
|         |            |   layers   |   Neurons  |   layers   |   Neurons  |         |        |
+=========+============+============+============+============+============+---------+--------+
|    5    |     64     |     3      |     512    |      2     |     10     |  94.38  | 1.5789 |
+---------+------------+------------+------------+------------+------------+---------+--------+
|    5    |     64     |     3      |     1024   |      2     |     10     |  95.08  | 1.5719 |
+---------+------------+------------+------------+------------+------------+---------+--------+
|    5    |     64     |     4      |     2048   |      2     |     10     |  95.36  | 1.5666 |
+---------+------------+------------+------------+------------+------------+---------+--------+
|    5    |     32     |     4      |     2048   |      2     |     10     |  95.53  | 1.5554 |
+---------+------------+------------+------------+------------+------------+---------+--------+

* After removing batch normalization from second maxout and adding normalization to input

+---------+------------+-------------------------+-------------------------+---------+--------+
|         |            |       Layer1            |       Layer2            |         |        |
|Training | Batch size +------------+------------+------------+------------+ Accuracy|  Loss  |
| Epochs  |            |  Number of |  Number of |  Number of |  Number of |   (%)   |        |
|         |            |   layers   |   Neurons  |   layers   |   Neurons  |         |        |
+=========+============+============+============+============+============+---------+--------+
|    5    |     64     |     3      |    1024    |      2     |     10     |  95.11  | 1.5243 |
+---------+------------+------------+------------+------------+------------+---------+--------+
|    5    |     64     |     3      |    2048    |      2     |     10     |  95.28  | 1.5216 |
+---------+------------+------------+------------+------------+------------+---------+--------+
|    5    |     64     |     4      |    1024    |      2     |     10     |  95.28  | 1.5234 |
+---------+------------+------------+------------+------------+------------+---------+--------+
|    5    |     64     |     4      |    2048    |      2     |     10     |  95.10  | 1.5683 |
+---------+------------+------------+------------+------------+------------+---------+--------+

As the accuracies and loss are coming nearly same in both cases. To simplify the network I've
removed normalization from first layer and added batch normalizations to two of the maxout
layers as before. The dropout I've kept as 0.5
