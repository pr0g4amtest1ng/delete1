This is an attempt to replicate the following paper as the hyperparameter link is not working in the paper.

arXiv:1302.4389_ **[stat.ML]**

.. _arXiv:1302.4389: https://arxiv.org/abs/1302.4389

======================
Hyperparameters Tuning
======================

--------
Training
--------

I've set by mistakenly dropout as 0.2. Somewhere I've read dropout should be less for input
channels. But I performed less dropout on weight vectors.

**Learning rate**: 0.001

+--------+------------+-------------------------+-------------------------+---------+--------+
|        |            |       Layer1            |       Layer2            |         |        |
| Epochs | Batch size +------------+------------+------------+------------+ Accuracy|  Loss  |
|        |            |  Number of |  Number of |  Number of |  Number of |   (%)   |        |
|        |            |   layers   |   Neurons  |   layers   |   Neurons  |         |        |
+========+============+============+============+============+============+=========+========+
|    5   |     64     |     3      |    512     |      2     |     10     |  93.31  | 1.6068 |
+--------+------------+------------+------------+------------+------------+---------+--------+
|    5   |     64     |     3      |    1024    |      2     |     10     |  93.87  | 1.5971 |
+--------+------------+------------+------------+------------+------------+---------+--------+
|    5   |     64     |     4      |    2048    |      2     |     10     |  94.31  | 1.5901 |
+--------+------------+------------+------------+------------+------------+---------+--------+
|    5   |     32     |     4      |    2048    |      2     |     10     |  94.75  | 1.5656 |
+--------+------------+------------+------------+------------+------------+---------+--------+

* After removing batch normalization from second maxout and adding normalization to input

+--------+------------+-------------------------+-------------------------+---------+--------+
|        |            |       Layer1            |       Layer2            |         |        |
| Epochs | Batch size +------------+------------+------------+------------+ Accuracy|  Loss  |
|        |            |  Number of |  Number of |  Number of |  Number of |   (%)   |        |
|        |            |   layers   |   Neurons  |   layers   |   Neurons  |         |        |
+========+============+============+============+============+============+=========+========+
|    5   |     64     |     3      |    1024    |      2     |     10     |  94.38  | 1.5355 |
+--------+------------+------------+------------+------------+------------+---------+--------+
|    5   |     64     |     3      |    2048    |      2     |     10     |  94.65  | 1.5307 |
+--------+------------+------------+------------+------------+------------+---------+--------+
|    5   |     64     |     4      |    1024    |      2     |     10     |  94.33  | 1.5242 |
+--------+------------+------------+------------+------------+------------+---------+--------+
|    5   |     64     |     4      |    2048    |      2     |     10     |  94.33  | 1.5911 |
+--------+------------+------------+------------+------------+------------+---------+--------+
